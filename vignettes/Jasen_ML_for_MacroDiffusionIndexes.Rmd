---
title: "**Machine Learning for Macro Diffusion Indexes**"
description: >
  A brief vignette explaining the process of forecasting Macroeconomic timeseries   using both traditional method and applying Machine Learning Algorithmns by 
  constructing Diffusion Indexes.
header-includes:
- \usepackage[default]{sourcesanspro}
- \usepackage[T1]{fontenc}
output: rmarkdown::html_vignette
documentclass: extarticle
fontsize: 12pt
geometry: margin=2cm
mainfont: SourceSansPro
vignette: >
  %\VignetteIndexEntry{**Machine Learning for Macro Diffusion Indexes**}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

### Introduction

Macroeconomics (from the Greek prefix makro- meaning "large" + economics) is a branch of economics dealing with the performance, structure, behavior, and decision-making of an economy as a whole. For example, using interest rates, taxes and government spending to regulate an economy's growth and stability.

The main goal of the project is to help macroeconomists to obtain useful insights from a dataset as a whole with the help of useful Machine Learning Algorithms by creating it's potential diffusion indexes.
Time series analysis is an area of statistics that focuses on analyzing time-dependent data. Time series can be analyzed either descriptively or inferentially. This has led to different approaches depending on the type of information available in time-series data.

We will be using the method of Vector Auto Regression for generating the forecasts initially and later proceed with the Machine Learning approach.
\

### Setting Up the R Environment

We will start by calling the required R libraries for Data parsing, processing and manipulation.
Here we have used two libraries, "quantmod" developed by [Joshua Ulrich](https://github.com/joshuaulrich).

```{r setup}
require(quantmod)
```

### Parsing the Data

Federal Reserve Economic Data (FRED) is an online database consisting of hundreds of thousands of economic data time series from scores of national, international, public, and private sources. FRED, created and maintained by the Research Department at the Federal Reserve Bank of St. Louis, goes far beyond simply providing data: It combines data with a powerful mix of tools that help the user understand, interact with, display, and disseminate the data. In essence, FRED helps users tell their data stories. 

First we'll create a new environment and a vector object containing the specific names(symbols) of the dataset we will be using.

```{r set_up_FRED_symbols}
#Create new environment
fundamental_data <- new.env()
behavioral_data <- new.env()
catalyst_data <- new.env()

### Add symbols
symbols1 <- c('AMTMNO', 'AMTMTI', 'OECDLOLITOAASTSAM', 'ICSA', 'INDPRO', 'T10Y2Y', 'BAA10Y', 'MKTGDPHKA646NWDB', 'MKTGDPSGA646NWDB', 'RGDPNABRA666NRUG', # Economic Trend
             'NFCI', # Liquidity
             'M2V', 'TOTCI', 'BOGZ1FA895050005Q' # Velocity
             )

symbols2 <- c('UMCSENT', 'CSUSHPISA', 'SPCS20RSA', # Confirmation Bias: Surveys
              'VIXCLS', 'VXVCLS', 'EVZCLS', 'THREEFYTP10', # Representative Bias
              'EMVOVERALLEMV', # Cognitive Dissonance
              'BOGZ1FA483064105Q' # Overconfidence
              )

symbols3 <- c('CFNAIMA3', 'STLFSI2', # Economic Surprise
              'WLEMUINDXD' # Geopolitics
              )
```

Now we will be using the 'getSymbols()' function of the 'quantmod' package in order to import the required dataset from fred.
The downloaded data will be stored in the form of xts timeseries object in the specified environment.

```{r get_symbols_FRED}
### Get data
getSymbols(Symbols = symbols1,
           src='FRED',
           env = fundamental_data)

getSymbols(Symbols = symbols2,
           src='FRED',
           env = behavioral_data)

getSymbols(Symbols = symbols3,
           src='FRED',
           env = catalyst_data)

# Clean-up
rm(symbols1)
rm(symbols2)
rm(symbols3)
```

```{r eval=FALSE}
# ## Get CSV Data
# 
# fundamental_csv <- new.env()
# behavioral_csv <- new.env()
# catalyst_csv <- new.env()
# 
# #fundamental
# # csvfiles <- list.files(path = "~/Documents/UserPath")
# csvfiles <- list.files(path = "~/Personal-Work/GitHub/Bloomberg-Dataset/GSOC_macro_Bloomberg_data/")
# # for(i in 1:length(csvfiles)) {
# #   temp = read.csv(paste0("~/Documents/UserPath",
# #                       csvfiles[i]))
# #   assign(x = paste0('data', i), value = temp, envir = fundamental_csv)
# # }
# for(i in 1:length(csvfiles)) {
#   temp = read.csv(paste0("~/Personal-Work/GitHub/Bloomberg-Dataset/GSOC_macro_Bloomberg_data/",
#                       csvfiles[i]))
#   assign(x = paste0('data', i), value = temp, envir = fundamental_csv)
# }
# 
# #behavioral
# csvfiles <- list.files(path = "~/Documents/UserPath")
# for(i in 1:length(csvfiles)) {
#   temp = read.csv(paste0("~/Documents/UserPath",
#                          csvfiles[i]))
#   #temp$Date <- strftime(strptime(temp$Date,"%m-%d-%Y"),"%Y-%m-%d")
#   temp$Date = as.Date(temp$Date, format ="%m-%d-%Y")
#   temp <- xts(temp[,2], order.by = as.Date(temp[,1], format = "%Y-%m-%d"))
#   assign(x = paste0('data', i), value = temp, envir = behavioral_csv)
# }
# 
# #catalyst
# csvfiles <- list.files(path = "~/Documents/UserPath")
# for(i in 1:length(csvfiles)) {
#   temp = read.csv(paste0("~/Documents/UserPath",
#                          csvfiles[i]))
#   assign(x = paste0('data', i), value = temp, envir = catalyst_csv)
# }
```

```{r merge_xts}
### Merge data - this will store it in a list
data_fundamental <- eapply(env = fundamental_data, FUN = merge.xts)
data_behavioral  <- eapply(env = behavioral_data, FUN = merge.xts)
data_catalyst    <- eapply(env = catalyst_data, FUN = merge.xts)
```

```{r check_periodicities}
## Check Periodicities
# Fundamental
ls_fun_periodicities <- lapply(data_fundamental, periodicity)
fun_periodicities <- as.data.frame(do.call(rbind, lapply(data_fundamental, periodicity)))
fun_periodicities$start <- as.Date(sapply(ls_fun_periodicities, "[[", 3))
fun_periodicities$end <- as.Date(sapply(ls_fun_periodicities, "[[", 4))
fun_periodicities

# Behavioral
ls_beh_periodicities <- lapply(data_behavioral, periodicity)
beh_periodicities <- as.data.frame(do.call(rbind, lapply(data_behavioral, periodicity)))
beh_periodicities$start <- as.Date(sapply(ls_beh_periodicities, "[[", 3))
beh_periodicities$end <- as.Date(sapply(ls_beh_periodicities, "[[", 4))
beh_periodicities

# Catalyst
ls_cat_periodicities <- lapply(data_catalyst, periodicity)
cat_periodicities <- as.data.frame(do.call(rbind, lapply(data_catalyst, periodicity)))
cat_periodicities$start <- as.Date(sapply(ls_cat_periodicities, "[[", 3))
cat_periodicities$end <- as.Date(sapply(ls_cat_periodicities, "[[", 4))
cat_periodicities

```


```{r convert_to_monthly}
data_fin_1 <- lapply(data_fundamental, to.monthly, OHLC=FALSE)
data_fin_2 <- lapply(data_behavioral, to.monthly, OHLC=FALSE)
data_fin_3 <- lapply(data_catalyst, to.monthly, OHLC=FALSE)

#clean-up
# rm(data_fundamental)
# rm(data_behavioral)
# rm(data_catalyst)
```


```{r merge_data}
#merge data
xts_data_fun <- do.call(merge.xts, data_fin_1)
xts_data_fun <- na.locf(xts_data_fun, na.rm = TRUE)

xts_data_beh <- do.call(merge.xts, data_fin_2)
xts_data_beh <- na.locf(xts_data_beh, na.rm = TRUE)

xts_data_cat <- do.call(merge.xts, data_fin_3)
xts_data_cat <- na.locf(xts_data_cat, na.rm = TRUE)

#clean-up
# rm(data_fin_1)
# rm(data_fin_2)
# rm(data_fin_3)
```

Complete cases chunk is unnecessary now that we use na.locf() with na.rm=TRUE for leading NAs. We will need to align the 3 categories...we do that next...

```{r complete_cases}
# #Include complete data
# idx_complete_1 <- which(complete.cases(xts_data_quart_1) == TRUE)
# idx_complete_2 <- which(complete.cases(xts_data_quart_2) == TRUE)
# idx_complete_3 <- which(complete.cases(xts_data_quart_3) == TRUE)
# 
# #Final trimed Datasets
# trim_data_fundamental = xts_data_quart_1[idx_complete_1]
# trim_data_behavioral = xts_data_quart_2[idx_complete_2]
# trim_data_catalyst = xts_data_quart_3[idx_complete_3]
# 
# #clean-up
# rm(xts_data_quart_1)
# rm(xts_data_quart_2)
# rm(xts_data_quart_3)
```

```{r check_data_start}
head(xts_data_fun)
head(xts_data_beh)
head(xts_data_cat)
```

So it would appear latest dataset starts Dec 2007. We will need to align all series to that start date, i think...

```{r algin_data}
xts_data_fun <- xts_data_fun["2007-12/"]
xts_data_beh <- xts_data_beh["2007-12/"]
xts_data_cat <- xts_data_cat["2007-12/"]
```

```{r DiffIdx_function_definition}
## Using SWFore to build Diffusion Indexes - Function
"DiffIdx" <- function(x,orig,m){
  ### Builds Stock and Watson's diffusion index prediction
  ### x: observed regressors
  ### orig: forecast origin
  ### m: selected number of PCs
  ###
  ### Output: Forecasts and MSE of forecasts (if data available)
  if(!is.matrix(x))x=as.matrix(x)
  nT=dim(x)[1]
  k=dim(x)[2]
  if(orig > nT)orig=nT
  if(m > k)m=k; if(m < 1)m=1
  # standardize the predictors
  x1=x[1:orig,]
  me=apply(x1,2,mean)
  se=sqrt(apply(x1,2,var))
  x1=x
  for (i in 1:k){
    x1[,i]=(x1[,i]-me[i])/se[i]
  }
  #
  V1=cov(x1[1:orig,])
  m1=eigen(V1)
  sdev=m1$values
  M=m1$vectors
  M1=M[,1:m]
  Dindex=x1%*%M1
  # y1=y[1:orig]
  # DF=Dindex[1:orig,]
  DF=Dindex
  # mm=lm(y1~DF)
  # coef=matrix(mm$coefficients,(m+1),1)
  # coef=matrix(mm$coefficients[-1],(m),1) # exclude the intercept
  #cat("coefficients: ","\n")
  #print(round(coef,4))
  yhat=NULL; MSE=NULL
  # if(orig < nT){
  #    newx=cbind(rep(1,(nT-orig)),Dindex[(orig+1):nT,])
  #    yhat=mm$coefficients[1]+(t(newx)%*%coef)
  #    err=y[(orig+1):nT]-yhat
  #    MSE=mean(err^2)
  #    cat("MSE of out-of-sample forecasts: ",MSE,"\n")
  # }

  DiffIdx <- xts(DF, index(x1))
}
```

```{r construct_DI}
#Constructing Diffusion Index

#fundamental
fundamental_DI <- DiffIdx(xts_data_fun, orig = nrow(xts_data_fun), m = 1)
head(fundamental_DI)

#behavioral
behavioral_DI <- DiffIdx(xts_data_beh, orig = nrow(xts_data_beh), m = 1)
head(behavioral_DI)

#catalyst
catalyst_DI <- DiffIdx(xts_data_cat, orig = nrow(xts_data_cat), m = 1)
head(catalyst_DI)
```

```{r plot_DI}
plot_data <- merge(fundamental_DI, behavioral_DI, catalyst_DI)
plot(plot_data, main = "Diffusion Index Plot")
addLegend("topright", on=1, 
          legend.names = c("Fundamental DI", "Behavioral DI", "Catalyst DI"),
          lty=c(1, 1), lwd=c(2, 1))

# # Ok, lets add S&P500
# ## Getting S&P 500 Data
# SP_500 <- getSymbols (Symbols = 'SPY', src= 'yahoo', auto.assign = FALSE)
# SP_500_period <- to.monthly(SP_500)
# # We get data starting Jan 2007 for the S&P 500, so we remove everything before Dec 2007, the start of the Diffusion Indexes
# SP_500_period <- SP_500_period["2007-12/"]
# 
# new_plot_data <- merge(plot_data, Ad(SP_500_period))
# plot(new_plot_data, main = "Diffusion Indexes overlayed with S&P500 - this doe not make sense to do actually")
```

Next we scale the Diffusion Indexes...

```{r}
#Scaling & Standardizing the observation
library(roll)
x <- plot_data # we can use plot_data...it is complete

scaled_DI <- roll_scale(x, width = 5, na_restore = FALSE)
```

Plot the scaled Diffusion Indexes...

```{r plot_scaled_DI}
plot(scaled_DI, main = "Diffusion Index Plot - scaled")
addLegend("topright", on=1, 
          legend.names = c("Fundamental DI", "Behavioral DI", "Catalyst DI"),
          lty=c(1, 1), lwd=c(2, 1))
```

Ok, its an ugly plot and i now need to think about this...

Ok, after some thought...there is really no constraint on the number of predictors we can pass the random forest model...so we should scale and lag all the things and pass them in...

```{r merge_input_series_and_lag}
x <- merge(plot_data, scaled_DI)
diff.lookbacks <- 1:12
x_lagged <- do.call(cbind, lapply(diff.lookbacks, function(n) lag(x, k = n)))
head(x_lagged, 12) # Use up to 12 lags...considering this is monthly data, 12m feels adequate
```

This is a TODO...get Y.

```{r get_Y}
SP_500 <- getSymbols (Symbols = 'SPY', src= 'yahoo', auto.assign = FALSE)
SP_500_period <- to.monthly(SP_500)
# We get data starting Jan 2007 for the S&P 500, so we remove everything before Dec 2007, the start of the Diffusion Indexes
SP_500_period <- SP_500_period["2007-12/"]
y <- SP_500_period

roll_scale(x2, width = 5, na_restore = FALSE)
```

